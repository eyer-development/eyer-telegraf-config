# Telegraf Configuration
[global_tags]

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "15s"
  round_interval = true
  metric_batch_size = 10000
  metric_buffer_limit = 100000
  collection_jitter = "0s"
  flush_interval = "15s"
  flush_jitter = "15s"
  precision = ""
  logfile_rotation_interval = "1d"
  logfile_rotation_max_size = "1MB"
  hostname = ""
# omit_hostname = false

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# # A plugin that can transmit metrics over HTTP
 [[outputs.http]]
   ## URL is the address to send metrics to
   url = "https://data.eyer.ai/api/telegraf/prometheus"
   timeout = "5s"
   method = "POST"
   data_format = "json"
   use_batch_format = true
    [outputs.http.headers]
        authenticate = "your_apiTokenPush"

# # Send telegraf metrics to file(s)
#[[outputs.file]]
#  ## Files to write to, "stdout" is a specially handled file.
#   files = ["stdout", "prometheus_new.txt"]
#   data_format = "json"

   
###############################################################################
#                            PROCESSOR PLUGINS                               #
###############################################################################

[[processors.starlark]]
  source = '''
# Global state to store previous counter values
state = {}

def apply(metric):
    results = []
    for field_name, field_value in metric.fields.items():
        new_metric = Metric(metric.name)
        
        # Determine metric type
        metric_type = "gauge"
        is_counter = False
        
        if "_total" in field_name or "_count" in field_name or "_sum" in field_name:
            metric_type = "counter"
            is_counter = True
        elif "_bytes" in field_name:
            metric_type = "bytes"
        elif "_seconds" in field_name:
            metric_type = "seconds"
        elif "_percent" in field_name:
            metric_type = "percent"
        
        # Copy all tags
        for tag_key, tag_value in metric.tags.items():
            new_metric.tags[tag_key] = tag_value
        
        # Add metric metadata
        new_metric.tags['metric_name'] = field_name
        new_metric.tags['metric_type'] = metric_type
        new_metric.time = metric.time
        
        # Calculate diff for counters, pass through for others
        if is_counter:
            # Create unique key for this counter
            key_parts = [
                metric.tags.get('url', ''),
                metric.tags.get('host', ''),
                field_name
            ]
            # Add dimensional tags (cpu, mode, device, etc.)
            for tag_key in sorted(metric.tags.keys()):
                if tag_key not in ['url', 'host', 'metric_name', 'metric_type']:
                    key_parts.append(str(metric.tags.get(tag_key, '')))
            
            metric_key = "|".join(key_parts)
            
            # Calculate diff if we have previous value
            if metric_key in state:
                prev_value = state[metric_key]
                # Handle counter resets
                if field_value >= prev_value:
                    diff = field_value - prev_value
                else:
                    # Counter reset detected
                    diff = field_value
                new_metric.fields['metric_value'] = diff
            else:
                # First reading - use current value
                new_metric.fields['metric_value'] = field_value
            
            # Store current value for next iteration
            state[metric_key] = field_value
        else:
            # Non-counters: use value as-is
            new_metric.fields['metric_value'] = field_value
        
        results.append(new_metric)
    
    return results
'''



###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# # Read metrics from one or many prometheus clients
 [[inputs.prometheus]]
#   ## An array of urls to scrape metrics from.
   urls = ["http://localhost:9100/metrics","http://localhost:9115/metrics"]
#
#   ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.
#   ## See "Metric Format Configuration" in plugins/inputs/prometheus/README.md for details.
#   ## Valid options: 1, 2
    metric_version = 2
#
#   ## Url tag name (tag containing scrapped url. optional, default is "url")
#   # url_tag = "url"
#
#   ## Whether the timestamp of the scraped metrics will be ignored.
#   ## If set to true, the gather time will be used.
#   # ignore_timestamp = false
#
#   ## An array of Kubernetes services to scrape metrics from.
#   # kubernetes_services = ["http://my-service-dns.my-namespace:9100/metrics"]
#
#   ## Kubernetes config file to create client from.
#   # kube_config = "/path/to/kubernetes.config"
#
#   ## Scrape Pods
#   ## Enable scraping of k8s pods. Further settings as to which pods to scape
#   ## are determiend by the 'method' option below. When enabled, the default is
#   ## to use annotations to determine whether to scrape or not.
#   # monitor_kubernetes_pods = false
#
#   ## Scrape Pods Method
#   ## annotations: default, looks for specific pod annotations documented below
#   ## settings: only look for pods matching the settings provided, not
#   ##   annotations
#   ## settings+annotations: looks at pods that match annotations using the user
#   ##   defined settings
#   # monitor_kubernetes_pods_method = "annotations"
#
#   ## Scrape Pods 'annotations' method options
#   ## If set method is set to 'annotations' or 'settings+annotations', these
#   ## annotation flags are looked for:
#   ## - prometheus.io/scrape: Required to enable scraping for this pod. Can also
#   ##     use 'prometheus.io/scrape=false' annotation to opt-out entirely.
#   ## - prometheus.io/scheme: If the metrics endpoint is secured then you will
#   ##     need to set this to 'https' & most likely set the tls config
#   ## - prometheus.io/path: If the metrics path is not /metrics, define it with
#   ##     this annotation
#   ## - prometheus.io/port: If port is not 9102 use this annotation
#
#   ## Scrape Pods 'settings' method options
#   ## When using 'settings' or 'settings+annotations', the default values for
#   ## annotations can be modified using with the following options:
#   # monitor_kubernetes_pods_scheme = "http"
#   # monitor_kubernetes_pods_port = "9102"
#   # monitor_kubernetes_pods_path = "/metrics"
#
#   ## Get the list of pods to scrape with either the scope of
#   ## - cluster: the kubernetes watch api (default, no need to specify)
#   ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.
#   # pod_scrape_scope = "cluster"
#
#   ## Only for node scrape scope: node IP of the node that telegraf is running on.
#   ## Either this config or the environment variable NODE_IP must be set.
#   # node_ip = "10.180.1.1"
#
#   ## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.
#   ## Default is 60 seconds.
#   # pod_scrape_interval = 60
#
#   ## Restricts Kubernetes monitoring to a single namespace
#   ##   ex: monitor_kubernetes_pods_namespace = "default"
#   # monitor_kubernetes_pods_namespace = ""
#   ## The name of the label for the pod that is being scraped.
#   ## Default is 'namespace' but this can conflict with metrics that have the label 'namespace'
#   # pod_namespace_label_name = "namespace"
#   # label selector to target pods which have the label
#   # kubernetes_label_selector = "env=dev,app=nginx"
#   # field selector to target pods
#   # eg. To scrape pods on a specific node
#   # kubernetes_field_selector = "spec.nodeName=$HOSTNAME"
#
#   ## Filter which pod annotations and labels will be added to metric tags
#   #
#   # pod_annotation_include = ["annotation-key-1"]
#   # pod_annotation_exclude = ["exclude-me"]
#   # pod_label_include = ["label-key-1"]
#   # pod_label_exclude = ["exclude-me"]
#
#   # cache refresh interval to set the interval for re-sync of pods list.
#   # Default is 60 minutes.
#   # cache_refresh_interval = 60
#
#   ## Scrape Services available in Consul Catalog
#   # [inputs.prometheus.consul]
#   #   enabled = true
#   #   agent = "http://localhost:8500"
#   #   query_interval = "5m"
#
#   #   [[inputs.prometheus.consul.query]]
#   #     name = "a service name"
#   #     tag = "a service tag"
#   #     url = 'http://{{if ne .ServiceAddress ""}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'
#   #     [inputs.prometheus.consul.query.tags]
#   #       host = "{{.Node}}"
#
#   ## Use bearer token for authorization. ('bearer_token' takes priority)
#   # bearer_token = "/path/to/bearer/token"
#   ## OR
#   # bearer_token_string = "abc_123"
#
#   ## HTTP Basic Authentication username and password. ('bearer_token' and
#   ## 'bearer_token_string' take priority)
#   # username = ""
#   # password = ""
#
#   ## Optional custom HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## Specify timeout duration for slower prometheus clients (default is 5s)
#   # timeout = "5s"
#
#   ## deprecated in 1.26; use the timeout option
#   # response_timeout = "5s"
#
#   ## HTTP Proxy support
#   # use_system_proxy = false
#   # http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile
#   # tls_cert = /path/to/certfile
#   # tls_key = /path/to/keyfile
#
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = "myhost.example.org"
#
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## Enable/disable TLS
#   ## Set to true/false to enforce TLS being enabled/disabled. If not set,
#   ## enable TLS only if any of the other options are specified.
#   # tls_enable = true
#
#   ## Control pod scraping based on pod namespace annotations
#   ## Pass and drop here act like tagpass and tagdrop, but instead
#   ## of filtering metrics they filters pod candidates for scraping
#   #[inputs.prometheus.namespace_annotation_pass]
#   # annotation_key = ["value1", "value2"]
#   #[inputs.prometheus.namespace_annotation_drop]
#   # some_annotation_key = ["dont-scrape"]
