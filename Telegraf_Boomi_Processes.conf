# Telegraf Configuration
[global_tags]

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "60s"
  round_interval = true
  metric_batch_size = 10000
  metric_buffer_limit = 100000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "5s"
  precision = ""
  logfile_rotation_interval = "1d"
  logfile_rotation_max_size = "1MB"
  hostname = ""
  omit_hostname = false


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# # A plugin that can transmit metrics over HTTP (normal processes)
 [[outputs.http]]
   url = "https://data.eyer.ai/api/telegraf/boomiprocesses"
    timeout = "10s"
    namepass = ["regular"]
    method = "POST"
    data_format = "json"
    use_batch_format = true
    [outputs.http.headers]
    authenticate = "your_API_push_key_goes_here"


# # A plugin that can transmit metrics over HTTP (low latency processes)
 [[outputs.http]]
   url = "https://data.eyer.ai/api/telegraf/boomilowlatencyprocesses"
    timeout = "10s"
    namepass = ["lowLatency"]
    method = "POST"
    data_format = "json"
    use_batch_format = true
    [outputs.http.headers]
    authenticate = "your_API_push_key_goes_here"



###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################

# # Dates measurements, tags, and fields that pass through this filter.
 [[processors.date]]
    namepass = ["lowLatency"]
    field_key = "unix"
    date_format = "unix"
    date_offset = "-480s"
    timezone = "UTC"

# # Keep the aggregate basicstats of each metric passing through.
 [[aggregators.basicstats]]
   namepass = ["regular"]
   period = "60s"
   drop_original = true
   stats = ["count","max","mean"]

# # Keep the aggregate basicstats of each metric passing through.
 [[aggregators.basicstats]]
   namepass = ["lowLatency"]
   period = "60s"
   drop_original = true
   stats = ["mean"]

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# # Read metrics from one or more commands that can output to stdout
 [[inputs.exec]]
   commands = ['powershell -command "& { . your_Telegraf_path\BoomiProcesses.ps1; RegularProcess }"']
   timeout = "15s"

# # Parse a complete file each interval (note the forward slashes in the path)
 [[inputs.file]]
 files = ["your_Telegraf_path/regular*.json"]
 character_encoding = "utf-16le"
 data_format = "json_v2"
 name_override = "regular"

# Add a subtable to use the `json_v2` parser
 [[inputs.file.json_v2]]
   [[inputs.file.json_v2.object]]
   namepass = ["regular"]
   path = "result"
   tags = ["processName","atomId"]





# # Read metrics from one or more commands that can output to stdout
 [[inputs.exec]]
   collection_offset = "5s"
   commands = ['powershell -command "& { . your_Telegraf_path\BoomiProcesses.ps1; LowLatencyProcess }"']
   timeout = "15s"

# # Parse a complete file each interval (note the forward slashes in the path)
 [[inputs.file]]
 collection_offset = "5s"
 files = ["your_Telegraf_path/lowLatency*.json"]
 character_encoding = "utf-16le"
 data_format = "json_v2"
 name_override = "lowLatency"

# Add a subtable to use the `json_v2` parser
 [[inputs.file.json_v2]]
   [[inputs.file.json_v2.object]]
   namepass = ["lowLatency"]
   path = "result"
   tags = ["processName","atomID","timeBlock"]
